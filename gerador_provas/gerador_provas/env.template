# ============================================================================
# GERADOR DE PROVAS - CONFIGURAÇÕES DE AMBIENTE
# ============================================================================
# INSTRUÇÕES:
# 1. Copie este arquivo para .env: copy env.template .env
# 2. Ajuste os valores conforme necessário
# 3. NUNCA commite o arquivo .env no repositório!
# ============================================================================

# ----------------------------------------------------------------------------
# FLASK
# ----------------------------------------------------------------------------
FLASK_APP=app.py
FLASK_ENV=development
FLASK_DEBUG=True
SECRET_KEY=sua-chave-secreta-muito-segura-aqui-123456

# ----------------------------------------------------------------------------
# SERVIDOR
# ----------------------------------------------------------------------------
HOST=0.0.0.0
PORT=5000

# ----------------------------------------------------------------------------
# BANCO DE DADOS (PostgreSQL)
# ----------------------------------------------------------------------------
# Para desenvolvimento LOCAL (sem Docker):
# DATABASE_URL=postgresql://provas_user:provas_password_2024@localhost:5432/provas_db
# POSTGRES_HOST=localhost

# Para Docker/Podman (usa o nome do serviço 'db'):
DATABASE_URL=postgresql://provas_user:provas_password_2024@db:5432/provas_db
POSTGRES_USER=provas_user
POSTGRES_PASSWORD=provas_password_2024
POSTGRES_DB=provas_db
POSTGRES_HOST=db
POSTGRES_PORT=5432

# ----------------------------------------------------------------------------
# CREWAI / LLM - CONFIGURAÇÃO DE INTELIGÊNCIA ARTIFICIAL
# ----------------------------------------------------------------------------
#
#  ╔═══════════════════════════════════════════════════════════════════════╗
#  ║                    DETECÇÃO AUTOMÁTICA DE PROVIDER                    ║
#  ╠═══════════════════════════════════════════════════════════════════════╣
#  ║  O sistema detecta automaticamente qual IA usar:                      ║
#  ║                                                                        ║
#  ║  1. Se OPENAI_API_KEY está configurada → usa OpenAI (ChatGPT)        ║
#  ║  2. Se ANTHROPIC_API_KEY está configurada → usa Claude               ║
#  ║  3. Senão → usa Ollama local (gratuito)                              ║
#  ║                                                                        ║
#  ║  Basta configurar a API key e o sistema usa automaticamente!         ║
#  ╚═══════════════════════════════════════════════════════════════════════╝
#
# ----------------------------------------------------------------------------
# MODO DE GERAÇÃO
# ----------------------------------------------------------------------------
# true = Usa IA real para gerar questões originais
# false = Usa templates pré-definidos (mais rápido, sem IA)
USE_AI_GENERATION=true

# ----------------------------------------------------------------------------
# OPÇÃO 1: DETECÇÃO AUTOMÁTICA (RECOMENDADO)
# ----------------------------------------------------------------------------
# Deixe LLM_PROVIDER=auto e configure apenas a API key que você tem:
LLM_PROVIDER=auto

# ⭐ GOOGLE GEMINI (GRATUITO!) - Recomendado para começar
# Obtenha em: https://aistudio.google.com/apikey
# GOOGLE_API_KEY=sua-chave-gemini-aqui

# Se você tem OpenAI (ChatGPT), descomente a linha abaixo:
# OPENAI_API_KEY=sk-sua-chave-openai-aqui

# Se você tem Anthropic (Claude), descomente a linha abaixo:
# ANTHROPIC_API_KEY=sua-chave-anthropic-aqui

# Se nenhuma API key estiver configurada, usa Ollama local automaticamente

# ----------------------------------------------------------------------------
# OPÇÃO 2: OLLAMA LOCAL (Gratuito, Privado)
# ----------------------------------------------------------------------------
# Para forçar o uso do Ollama mesmo tendo API keys configuradas:
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2
OLLAMA_BASE_URL=http://localhost:11434

# Instale Ollama: https://ollama.ai
# Baixe um modelo: ollama pull llama3.2
#
# Modelos recomendados:
# - llama3.2 (leve, rápido, 3GB)
# - llama3.1:8b (equilibrado, 8GB)
# - mistral (bom raciocínio, 7GB)
# - mixtral (mais potente, 26GB)

# ----------------------------------------------------------------------------
# OPÇÃO 3: FORÇAR PROVIDER ESPECÍFICO
# ----------------------------------------------------------------------------
# Se quiser forçar um provider específico, configure:
#
# Para OpenAI:
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# OPENAI_API_KEY=sk-...
#
# Para Anthropic:
# LLM_PROVIDER=anthropic
# LLM_MODEL=claude-3-sonnet-20240229
# ANTHROPIC_API_KEY=...
#
# Para Ollama:
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2

# ----------------------------------------------------------------------------
# MODELOS RECOMENDADOS POR PROVIDER
# ----------------------------------------------------------------------------
# ⭐ Google Gemini (GRATUITO!):
#   - gemini-1.5-flash (rápido, gratuito) ← RECOMENDADO
#   - gemini-1.5-flash-8b (mais leve, gratuito)
#   - gemini-1.5-pro (mais potente, limites menores)
#
# OpenAI:
#   - gpt-4o-mini (rápido, barato, bom)
#   - gpt-4o (mais potente, mais caro)
#   - gpt-3.5-turbo (mais barato, menos potente)
#
# Anthropic:
#   - claude-3-haiku (rápido, barato)
#   - claude-3-sonnet (equilibrado)
#   - claude-3-opus (mais potente, mais caro)
#
# Ollama (local, gratuito):
#   - llama3.2 (leve) ← RECOMENDADO para início
#   - llama3.1:8b (melhor qualidade)
#   - mistral (bom para raciocínio)

# ----------------------------------------------------------------------------
# DIAGRAMAS
# ----------------------------------------------------------------------------
DIAGRAMAS_DIR=static/diagramas
DIAGRAMAS_DPI=150

# ----------------------------------------------------------------------------
# LOGS
# ----------------------------------------------------------------------------
LOG_LEVEL=INFO
LOG_DIR=logs

# ----------------------------------------------------------------------------
# EXPORTAÇÃO
# ----------------------------------------------------------------------------
OUTPUT_DIR=output
PDF_OUTPUT_DIR=output/pdf
LATEX_OUTPUT_DIR=output/latex

# ----------------------------------------------------------------------------
# CACHE (Opcional - para produção)
# ----------------------------------------------------------------------------
# REDIS_URL=redis://localhost:6379/0
# CACHE_TYPE=redis
# CACHE_DEFAULT_TIMEOUT=300
